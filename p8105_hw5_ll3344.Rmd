---
title: "p8105_hw5_ll3344"
author: "Lusha Liang"
output: github_document
---

## Problem 1: Unsolved Homicides

Load necessary packages.

```{r}
library(tidyverse)
library(viridis)
library(plotly)
library(patchwork)
```

Set ggplot theme. 

```{r}
# Minimalist theme
theme_set(theme_minimal())
```

Read in the data.
```{r}
homicide_df = 
  read_csv("./data/homicide_data.csv")
```

Describe the raw data:

* The homicide data set contains `r nrow(homicide_df)` rows, representing homicides in 50 cities across America and `r ncol(homicide_df)` columns which include information about the homicide. 
* The information contained about each homicide includes: `r colnames(homicide_df)`. 
* The column disposition contains three possible outcomes: `r homicide_df %>% pull(disposition) %>% as.factor() %>% levels()`.
* The dates included range from 2007-2017.

Next, we will create a city_state variable that combines the city and state in one variable. We will then combine the disposition "Closed without arrest" and "Open/No arrest" into one disposition of "unsolved." The remaining "Closed by arrest" homicides will be designated as "solved." 

```{r}
homicide_df = 
  read_csv("./data/homicide_data.csv") %>%
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>%
  select(city_state, resolved) %>%
  filter(city_state != "Tulsa_AL")
  
```

We will now group by city_state variable and count total and unsolved homicides. 

```{r}
aggregate_df = 
  homicide_df %>%
  group_by(city_state) %>%
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  ) 
```

Now we will perform a prop test for a single city to estimate the proportion of unsolved homicides. 

```{r}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved),
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy()
```

Next, we will iterate the prop. test for multiple cities at once.

```{r}
results_df = 
  aggregate_df %>%
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>%
  select(-prop_tests) %>%
  unnest(tidy_tests) %>%
  select(city_state, estimate, conf.low, conf.high)
```

We can plot the estimates and confidence intervals for proportion of unsolved homicides for each city. 

```{r}
results_df %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) 
```

## Problem 2: Longitudinal Study Data 

Load and tidy the data. This involves compiling each spreadsheet into a list while keeping the title of the spreadsheet, since this also contains information about the subject ID and the control vs experimental group. Then to make the dataset tidy we pivot_longer so that the week number and values obtained are in contained in separate columns. 

```{r}
lga_df = 
  tibble(
    path = list.files("prob_2"),
  ) %>% 
  mutate(
    path = str_c("prob_2/", path),
    data = map(path, read_csv)
    ) %>%
  unnest(data) %>%
  mutate(path = str_replace(path, "prob_2/", " "),
         path = str_replace(path, ".csv", " ")) %>%
  separate(path, into = c("group", "id"), sep = "_") %>%
  pivot_longer(
    week_1:week_8,
    names_to = "week", 
    names_prefix = "week_", 
    values_to = "observations"
  ) %>%
  mutate(
    group = str_replace(group, "con", "control"),
    group = str_replace(group, "exp", "experimental")
  ) %>%
  relocate(id) %>%
  mutate(
    group = as.factor(group),
    week = as.numeric(week)
  )
```

Create a spaghetti plot showing observations on each subject over time.

```{r}
lga_df %>%
  ggplot(aes(x = week, y = observations, color = id)) +
  geom_point() +
  geom_path() +
  facet_grid(.~group) +
  scale_colour_viridis_d()

# We could also make a plot-ly plot:
#lga_df %>%
#   mutate(text_label = str_c("Study ID: ", id, "\nGroup:", group)) %>%
#   plot_ly(
#     x = ~week, y = ~observations, color = ~id,
#     colors = "viridis", type = "scatter",
#     mode = "line", text = ~text_label
#   )
```

In general, the experimental arm has higher values for the observations and seems to increase over time whereas the control arm observations stayed relatively constant over time. This makes sense as most likely the control group was not receiving any intervention. 

## Problem 3: Power

First, we will generate 5000 datasets from a normal distribution with n = 30, mean = 0, and standard deviation = 5. Next, we will create a function that outputs the estimate and p value from running a two-sided t-test with the null hypothesis of mu = 0.

```{r}
set.seed(1)

sim_mean_p = function(n = 30, mean, sd = 5) {
  
  sim_data = tibble(
    x = rnorm(n = n, mean, sd = sd),
  ) %>%
    t.test() %>%
    broom::tidy() %>%
    select(estimate, p.value) %>%
    rename(sample_mean = estimate)
}

sim_results = 
  rerun(5000, sim_mean_p(mean = 0)) %>%
  bind_rows()
  
```

We will now repeat the above process for different means between 1-6. 

```{r}
mean_list = 
  list(
    "mean_0" = 0,
    "mean_1" = 1,
    "mean_2" = 2,
    "mean_3" = 3,
    "mean_4" = 4,
    "mean_5" = 5,
    "mean_6" = 6
  )

sim_vary_means = 
  tibble(diff_means = c(0, 1, 2, 3, 4, 5, 6)) %>%
  mutate(
    output_lists = map(.x = diff_means, ~rerun(5000, sim_mean_p(mean = .x))),
    estimate_dfs = map(output_lists, bind_rows)) %>%
      select(-output_lists) %>%
      unnest(estimate_dfs)
```

Finally, we will plot the proportions of times the null was rejected versus true mu. The plot shows that as the true mean increases away from 0 (effect size increases), the proportion of times the null was rejected (the power) increases. At an effect size of around 4, the power approaches 1. 

```{r}
sim_vary_means %>%
  filter(p.value < 0.05) %>%
  group_by(diff_means) %>%
  count() %>%
  mutate(prop = n/5000) %>%
  ggplot(aes(x = diff_means, y = prop)) +
  geom_line() +
  geom_point() + 
  xlab("True Means") + 
  ylab("Proportion rejection")
```

Now we will make a plot showing the average estimate of the mean on the y axis and the true value of mu on the x axis. 
The second plot shows the average estimate of the mean on the y axis only in samples for which the null was rejected and the true value of mu on the x axis. 

```{r}
all_means_p = 
  sim_vary_means %>%
  mutate(
    diff_means = str_c("mean = ", diff_means)
  ) %>%
  ggplot(aes(x = diff_means, y = sample_mean, fill = diff_means)) +
  geom_violin(alpha = .5) +
  xlab("True Means") + 
  ylab("Sample Means") +
  theme(legend.position = "none") + 
  ggtitle("All Means") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
  stat_summary(fun = "mean", color = "black")

rejected_means_p =
  sim_vary_means %>%
  mutate(
    diff_means = str_c("mean = ", diff_means)
  ) %>%
  filter(p.value < 0.05) %>%
  ggplot(aes(x = diff_means, y = sample_mean, fill = diff_means)) +
  geom_violin(alpha = .5) + 
  xlab("True Means") + 
  ylab("Sample Means") +
  theme(legend.position = "none") + 
  ggtitle("Rejected Means") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
  stat_summary(fun = "mean", color = "black")

all_means_p + rejected_means_p
```

The second plot shows that for true mean = 0, the rejected values are either significantly larger or smaller than zero but the overall mean is 0. This makes sense as these are the values at the extreme ends of the normal distribution but they are still centered around 0. For smaller true means > 0, the sample average of mu across tests for which the null is rejected is higher than the true mean. However, as the true mean gets larger (and the effect size increases), the power to reject increases to almost 1 and nearly all the samples are rejected. Thus the rejected means and the true means converge. 

